# Nano AI — Technical Debrief

## Executive Summary

**Nano AI** is a fully offline, browser-based application that leverages Chrome's built-in Gemini Nano model for local AI inference. It provides two main capabilities: a conversational chatbot and a sentiment classifier — both running entirely on-device with zero network dependency once the model is cached.

This document provides a comprehensive technical breakdown of the architecture, implementation details, and features.

---

## Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Chrome Built-in AI API](#chrome-built-in-ai-api)
4. [Features](#features)
5. [File Structure](#file-structure)
6. [Implementation Details](#implementation-details)
7. [Offline Capability](#offline-capability)
8. [Setup Requirements](#setup-requirements)
9. [Limitations](#limitations)
10. [Future Enhancements](#future-enhancements)

---

## Overview

### What It Does

Nano AI is a web application that demonstrates the capabilities of Chrome's experimental Prompt API, which provides access to Gemini Nano — a lightweight large language model that runs entirely within the browser.

### Key Value Propositions

| Benefit | Description |
|---------|-------------|
| **Privacy** | All processing happens locally. No data ever leaves the user's device. |
| **Offline** | Works without internet once the model is downloaded (managed by Chrome). |
| **Zero Cost** | No API keys, no usage fees, no backend infrastructure required. |
| **Low Latency** | No network round-trips. Inference happens in milliseconds to seconds. |
| **Simple Deployment** | Static files only. Can be served from any web server or file system. |

### Target Use Cases

- Privacy-sensitive text analysis
- Offline-capable AI assistants
- Edge computing / air-gapped environments
- Educational demonstrations of local LLM capabilities
- Rapid prototyping without API costs

---

## Architecture

### High-Level Architecture

```
┌─────────────────────────────────────────────────────────────────────────┐
│                              Browser (Chrome 138+)                      │
│                                                                         │
│  ┌──────────────────────────────────────────────────────────────────┐  │
│  │                         Web Application                          │  │
│  │  ┌─────────────┐  ┌─────────────┐  ┌─────────────────────────┐  │  │
│  │  │   Chat UI   │  │ Classifier  │  │     Message Log         │  │  │
│  │  │   Panel     │  │   Panel     │  │       Panel             │  │  │
│  │  └──────┬──────┘  └──────┬──────┘  └────────────┬────────────┘  │  │
│  │         │                │                      │                │  │
│  │         └────────────────┼──────────────────────┘                │  │
│  │                          │                                       │  │
│  │                    ┌─────▼─────┐                                 │  │
│  │                    │   ai.js   │  ◄── Application Logic          │  │
│  │                    └─────┬─────┘                                 │  │
│  └──────────────────────────┼───────────────────────────────────────┘  │
│                             │                                          │
│  ┌──────────────────────────▼───────────────────────────────────────┐  │
│  │                    Chrome Built-in AI                            │  │
│  │  ┌─────────────────────────────────────────────────────────────┐ │  │
│  │  │                  LanguageModel API                          │ │  │
│  │  │   • LanguageModel.availability()                            │ │  │
│  │  │   • LanguageModel.create(options)                           │ │  │
│  │  │   • session.prompt(text, options)                           │ │  │
│  │  │   • session.promptStreaming(text, options)                  │ │  │
│  │  └─────────────────────────┬───────────────────────────────────┘ │  │
│  │                            │                                     │  │
│  │  ┌─────────────────────────▼───────────────────────────────────┐ │  │
│  │  │                    Gemini Nano Model                        │ │  │
│  │  │                    (~2GB, cached by Chrome)                 │ │  │
│  │  └─────────────────────────────────────────────────────────────┘ │  │
│  └──────────────────────────────────────────────────────────────────┘  │
└─────────────────────────────────────────────────────────────────────────┘
```

### Data Flow

```
User Input
    │
    ▼
┌─────────────────┐
│     UI Layer    │  (ui.js)
│  - Capture input│
│  - Update DOM   │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│   Application   │  (main.js)
│  - Route events │
│  - Manage state │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│    AI Layer     │  (ai.js)
│  - Session mgmt │
│  - Prompt craft │
│  - Parse output │
└────────┬────────┘
         │
         ▼
┌─────────────────┐
│  LanguageModel  │  (Chrome API)
│  - Local inference
│  - Streaming    │
└────────┬────────┘
         │
         ▼
    AI Response
         │
         ▼
┌─────────────────┐
│   Message Log   │  (Debugging/Audit)
└─────────────────┘
```

---

## Chrome Built-in AI API

### API Overview

Chrome's Prompt API exposes Gemini Nano through a global `LanguageModel` class. This is **not** `window.ai` (which was an older API surface) — the current implementation uses a direct global.

### Key API Methods

#### 1. Checking Availability

```javascript
const availability = await LanguageModel.availability();
// Returns: 'available' | 'downloadable' | 'downloading' | 'unavailable'
```

| Status | Meaning |
|--------|---------|
| `available` | Model is ready for immediate use |
| `downloadable` | Model can be downloaded (triggers on first `create()`) |
| `downloading` | Model is currently being downloaded |
| `unavailable` | Model cannot run on this device |

#### 2. Creating a Session

```javascript
const session = await LanguageModel.create({
  systemPrompt: 'You are a helpful assistant.',
  monitor: (m) => {
    m.addEventListener('downloadprogress', (e) => {
      console.log(`Download: ${Math.round(e.loaded * 100)}%`);
    });
  }
});
```

**Options:**
- `systemPrompt` — Sets the model's behavior/persona
- `monitor` — Callback for download progress events
- `temperature` — Controls randomness (0-2)
- `topK` — Controls diversity of token selection

#### 3. Running Inference (Non-Streaming)

```javascript
const response = await session.prompt('What is the capital of France?');
// Returns: 'The capital of France is Paris.'
```

#### 4. Running Inference (Streaming)

```javascript
const stream = session.promptStreaming('Tell me a story');

for await (const chunk of stream) {
  console.log(chunk); // Prints text as it's generated
}
```

**Note:** Chrome's streaming returns cumulative text (full response so far), not deltas. The implementation handles both modes.

#### 5. Structured Output (JSON Schema)

```javascript
const response = await session.prompt('Analyze this text', {
  responseConstraint: {
    type: 'object',
    properties: {
      sentiment: { type: 'string', enum: ['positive', 'negative', 'neutral'] },
      confidence: { type: 'number' }
    },
    required: ['sentiment', 'confidence']
  }
});
// Returns valid JSON matching the schema
```

This forces the model to output structured data, dramatically improving reliability for classification tasks.

---

## Features

### 1. Conversational Chat

| Feature | Description |
|---------|-------------|
| **Streaming Responses** | Text appears in real-time as the model generates it |
| **Context Preservation** | The session maintains conversation history |
| **System Prompt** | Model is configured as a "helpful assistant" |
| **Clear Chat** | Button to reset the conversation UI |

**Implementation Notes:**
- Uses `promptStreaming()` for real-time feedback
- Handles both delta and cumulative streaming modes
- Typing indicator shown while waiting for response

### 2. Sentiment Classifier

| Feature | Description |
|---------|-------------|
| **Structured Output** | Uses JSON Schema (`responseConstraint`) for reliable parsing |
| **Multi-class Scores** | Returns positive, negative, and neutral scores (sum to 1.0) |
| **Confidence Score** | Model's confidence in its classification |
| **Reasoning** | Model explains why it classified the text that way |
| **Visual Feedback** | Animated score bars and emoji-based sentiment badge |

**Output Format:**
```json
{
  "sentiment": "positive",
  "confidence": 0.9,
  "positive_score": 0.7,
  "negative_score": 0.1,
  "neutral_score": 0.2,
  "reasoning": "The text explicitly states 'prob more positive,' indicating..."
}
```

**Fallback Handling:**
- If `responseConstraint` fails, falls back to prompt-based JSON extraction
- If JSON parsing fails, uses keyword-based heuristics
- Graceful degradation ensures the app never crashes

### 3. Message Log (Debug Panel)

| Feature | Description |
|---------|-------------|
| **Complete Audit Trail** | Every message sent and received is logged |
| **Timestamps** | Millisecond-precision timestamps |
| **Message Types** | Color-coded: SENT (cyan), RECEIVED (green), ERROR (red), SYSTEM (purple) |
| **Metadata** | Shows inference time, structured output status, message type |
| **Truncation** | Long messages truncated with "[truncated]" indicator |

**Log Entry Types:**
- `sent` — User messages to the model
- `received` — Model responses
- `error` — Any failures or exceptions
- `system` — Initialization events, status changes

### 4. Offline Support

| Component | Caching Method |
|-----------|----------------|
| HTML, CSS, JS | Service Worker (app controls) |
| Gemini Nano Model | Chrome's internal cache (~2GB, Chrome controls) |

**Service Worker Strategy:**
- **Install:** Pre-caches all app assets
- **Fetch:** Cache-first for app assets, stale-while-revalidate for fonts
- **Activate:** Cleans up old cache versions

### 5. Responsive Design

| Breakpoint | Layout |
|------------|--------|
| > 1200px | 3 columns (Chat, Classifier, Log) |
| 768-1200px | 2 columns (Chat + Classifier, Log full-width) |
| < 768px | 1 column (stacked) |

### 6. 100% Offline Assets

**No external dependencies:**
- System fonts only (`-apple-system`, `Segoe UI`, `Roboto`, etc.)
- No Google Fonts
- No CDN scripts
- No external stylesheets

---

## File Structure

```
chrome-ai-classifier/
├── index.html              # Main HTML structure
├── styles.css              # All styling (CSS variables, responsive grid)
├── manifest.json           # PWA manifest for installability
├── service-worker.js       # Offline caching logic
├── src/
│   ├── ai.js               # Chrome AI API wrapper
│   ├── ui.js               # DOM manipulation and UI state
│   └── main.js             # Application bootstrap and event wiring
└── README.md               # Setup instructions
```

### File Responsibilities

#### `index.html`
- Semantic HTML structure
- Three-panel grid layout
- No inline JavaScript (clean separation)
- No external resource loading

#### `styles.css`
- CSS custom properties (variables) for theming
- Dark mode color scheme
- Responsive grid with media queries
- Animations (fade-in, pulse, progress bars)
- System font stack only

#### `ai.js`
- `NanoAI` global namespace
- Session management (chat + classifier)
- Message logging system
- `initAI()` — Initialize both sessions
- `chat()` — Non-streaming chat
- `chatStreaming()` — Streaming chat with callback
- `classifyText()` — Structured sentiment analysis
- `parseClassification()` — Robust response parsing with fallbacks

#### `ui.js`
- `NanoUI` global namespace
- DOM element caching
- Event handler setup
- `updateChatResponse()` — Handle streaming updates
- `showClassifierResults()` — Render classification output
- `addLogEntry()` — Add entries to message log
- Status indicators (AI status, network status)

#### `main.js`
- Application bootstrap
- Service Worker registration
- Event routing (connects UI callbacks to AI functions)
- State management (`state.aiReady`)
- Network status listeners

#### `service-worker.js`
- Pre-caches app shell on install
- Cache-first serving for offline support
- Old cache cleanup on activate

---

## Implementation Details

### Why Not ES Modules?

The code uses IIFEs (Immediately Invoked Function Expressions) instead of ES modules:

```javascript
window.NanoAI = (function() {
  // Private state
  let session = null;
  
  // Public API
  return {
    initAI: initAI,
    chat: chat
  };
})();
```

**Reason:** ES modules have a different scope than regular scripts. The `LanguageModel` global is injected by Chrome into the window scope, but ES modules don't automatically see window globals when using bare identifiers like `LanguageModel`. Using regular scripts avoids this scoping issue.

### Streaming Text Handling

Chrome's `promptStreaming()` returns **cumulative** text, not deltas:

```javascript
// Chrome returns:
chunk 1: "Hello"
chunk 2: "Hello, how"
chunk 3: "Hello, how are"
chunk 4: "Hello, how are you?"
```

The implementation detects this:

```javascript
if (chunk.length > fullResponse.length) {
  fullResponse = chunk;  // Cumulative mode
} else {
  fullResponse += chunk; // Delta mode (fallback)
}
```

### Structured Output Fallback Chain

The classifier uses a 3-tier fallback strategy:

```
1. responseConstraint (JSON Schema)
         │
         ▼ (if fails)
2. Prompt-based JSON extraction
         │
         ▼ (if fails)
3. Keyword heuristics
```

This ensures the app never fails to return a result, even if the model doesn't follow instructions perfectly.

### Message Logging Architecture

All AI communication flows through logging:

```javascript
function logMessage(type, content, metadata) {
  const entry = {
    type: type,
    content: content,
    metadata: metadata,
    timestamp: new Date()
  };
  messageLog.push(entry);
  
  // Notify UI via custom event
  window.dispatchEvent(new CustomEvent('ai-log', { detail: entry }));
}
```

The UI listens for these events:

```javascript
window.addEventListener('ai-log', function(e) {
  addLogEntry(e.detail);
});
```

This decouples the AI layer from the UI layer while maintaining a complete audit trail.

---

## Offline Capability

### Two-Layer Caching

| Layer | What's Cached | Who Controls It |
|-------|---------------|-----------------|
| App Cache | HTML, CSS, JS (~50KB) | Service Worker |
| Model Cache | Gemini Nano (~2GB) | Chrome |

### Service Worker Lifecycle

```
┌─────────────────────────────────────────────────────────────┐
│                      INSTALL                                │
│  • Pre-cache: index.html, styles.css, src/*.js              │
│  • Skip waiting to activate immediately                     │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                      ACTIVATE                               │
│  • Delete old caches (version mismatch)                     │
│  • Claim all clients                                        │
└─────────────────────────────────────────────────────────────┘
                              │
                              ▼
┌─────────────────────────────────────────────────────────────┐
│                       FETCH                                 │
│  • App assets: Cache-first                                  │
│  • External (fonts): Stale-while-revalidate                 │
└─────────────────────────────────────────────────────────────┘
```

### Offline Requirements

For full offline functionality, **both** conditions must be met:

1. ✅ App assets cached by Service Worker
2. ✅ Gemini Nano model downloaded by Chrome

The model download is triggered by the first `LanguageModel.create()` call and managed entirely by Chrome.

---

## Setup Requirements

### Browser Requirements

| Requirement | Minimum |
|-------------|---------|
| Browser | Chrome 138+ (Canary recommended) |
| OS | Windows 10/11, macOS 13+, Linux, ChromeOS |
| Storage | 22GB+ free space |
| GPU (optional) | 4GB+ VRAM |
| CPU (if no GPU) | 16GB RAM, 4+ cores |

### Chrome Flags

Enable these at `chrome://flags`:

| Flag | Setting |
|------|---------|
| `#optimization-guide-on-device-model` | Enabled BypassPerfRequirement |
| `#prompt-api-for-gemini-nano` | Enabled |
| `#prompt-api-for-gemini-nano-multimodal-input` | Enabled (optional) |

### Model Download

After enabling flags and restarting Chrome:

1. Visit `chrome://components`
2. Find "Optimization Guide On Device Model"
3. Click "Check for update"
4. Wait for download (~2GB)

Alternatively, the model downloads automatically on first `LanguageModel.create()` call.

---

## Limitations

### Model Limitations

| Limitation | Impact |
|------------|--------|
| Context window | ~4000 tokens (shorter than cloud models) |
| Factual accuracy | Not reliable for knowledge queries |
| Instruction following | May not always produce valid JSON |
| Speed | 1-25+ seconds per response depending on length |

### Platform Limitations

| Limitation | Impact |
|------------|--------|
| Chrome-only | Won't work in Firefox, Safari, Edge |
| Desktop-only | Mobile Chrome doesn't support Gemini Nano |
| Experimental API | May change or break between Chrome versions |
| No fine-tuning | Can't customize the model |

### Application Limitations

| Limitation | Workaround |
|------------|------------|
| No chat export | Could add download button for log |
| No persistent chat | Could use localStorage |
| Single conversation | Could add multi-chat support |
| No image input | Multimodal flag exists but not implemented |

---

## Future Enhancements

### Potential Additions

1. **Chat History Persistence**
   - Save conversations to localStorage
   - Export chat as JSON/Markdown

2. **Multiple Chat Sessions**
   - Tab interface for parallel conversations
   - Different system prompts per session

3. **Additional Classifiers**
   - Emotion detection (joy, sadness, anger, fear)
   - Topic classification
   - Language detection
   - Spam/ham classification

4. **Multimodal Input**
   - Image analysis (flag already exists)
   - Audio transcription

5. **Advanced Settings Panel**
   - Temperature slider
   - Top-K adjustment
   - Custom system prompts

6. **Performance Metrics**
   - Tokens per second
   - Memory usage
   - Session statistics

---

## Conclusion

Nano AI demonstrates that meaningful AI capabilities can run entirely in the browser, with no backend infrastructure and complete data privacy. While Chrome's Prompt API is still experimental, it represents a significant shift toward edge AI and opens new possibilities for privacy-preserving applications.

The implementation prioritizes:
- **Robustness** — Multiple fallback strategies for parsing
- **Transparency** — Full message logging for debugging
- **Simplicity** — No build tools, no dependencies, just static files
- **Offline-first** — Works without network once cached

This architecture can serve as a foundation for more sophisticated local AI applications as the Chrome Prompt API matures.

---

*Last Updated: January 2025*
*Chrome Version Tested: 138+ (Canary)*
*API Version: LanguageModel (Prompt API)*
